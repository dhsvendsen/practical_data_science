{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: A Data Scientist's most fundamental tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "The point of these exercises is to refresh your memory on some mathematics and get you comfortable doing computations in code.\n",
    "\n",
    "The exercises today cover:\n",
    "* Basic visualization\n",
    "* Linear algebra\n",
    "* Statistics\n",
    "\n",
    "**Advice**: Some of you may be new to solving problems using code. You may be wondering *what level of detail* I expect in your solutions, your code comments and explanations. **This is the guideline:** Solve the exercises in a manner that allows you to—later in life—use them as examples. This also means that you should add `#code comments` when the code isn't self-explanatory or if you're afraid it won't make sense when you look at it with fresh eyes. You may also want to comment on your output in plain text to capture the conclusions you arrive at throughout your analysis. But express yourself succinctly. To quote (probably) Einstein: \"*Make everything as simple as possible, but not simpler*\". Finally, when you optimize for your own future comprehension, other people will be able to understand what you did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.1.1**: The figure below meets the minumum style requirements which I expect the figures you make in this class (and life in general) should also meet:\n",
    "* Figure sizing. Try to make the aspect ratio close to 4:3.\n",
    "* Axis labels. Note that you may want to alter the `fontsize` to make them look nice.\n",
    "* Properly sized x and y tick labels.\n",
    "* Title (optional: not always necessary, but oftens helps the reader)\n",
    "* Legend (general rule: only use if you have multiple trends so reader can distinguish).\n",
    ">\n",
    "> Your task in this exercise if to reproduce this figure (perfect match not required).\n",
    ">\n",
    ">*Hint: To get figures to display inside the notebook, use the Jupyter magic `%matplotlib inline`. For pointers on how to make plots like this in Python, Google something like \"scatter plot python\" and see if you can find some examples of how other people do this.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dhsvendsen.github.io/images/ex211.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5592661 ,  0.21670406, -0.64389097, -1.43748149, -1.04738727,\n",
       "       -0.25641203,  1.49635567, -0.64611326, -0.98949272, -0.51116256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = np.random.randn(10)\n",
    "y = np.random.randn(10)\n",
    "plt.yscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.1.2**: The `get_x_y` function below gives you the number of comments versus score for the latest `N` posts on a given `subreddit`.\n",
    "1. Make a scatter plot of `x` vs. `y` for the \"blackmirror\" subreddit (**remember** what you learned in the previous exercise about **styling**). Comment on what you see.\n",
    "2. Maybe you've noticed that it looks pretty bad right? That's because the majority of the data is clustered at low values while some posts have very high values of both score *and* comments! This is a very common thing. To visualize it you should then try to *transform* it somehow. Which transformation would allow us to visualize both *high* and *low* values in one plot?\n",
    "3. In two seperate figures, floating side by side, scatter plot (left) the set of x and y variables for \"blackmirror\" and (right) x and y for \"news\". Remember to transform the data. My figure looks like [this](https://dhsvendsen.github.io/images/ex2123.pdf).\n",
    "4. Comment on any differences you see in the trends. Why might number of comments versus post upvotes look different for a TV-show than for world news?\n",
    ">\n",
    ">*Hint: By \"transformation\" I explicitly mean that you map some function onto every value in a list of values. Example: I can apply a square root transformation like `x = [np.sqrt(v) for v in x]`. A faster way to do that, of course, would be just `x = np.sqrt(x)`.*\n",
    ">*Note:* You can also use data tranformations to illustrate a nonlinear relationship in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T14:08:04.825646Z",
     "start_time": "2020-01-28T14:07:51.192395Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "\n",
    "def get_x_y(subreddit, N, count=25):\n",
    "    \n",
    "    def _get_data(subreddit, count, after):\n",
    "        url = \"https://www.reddit.com/r/%s/.json?count=%d&after=%s\" % (subreddit, count, after)\n",
    "        data = rq.get(url, headers = {'User-agent': 'sneakybot'}).json()\n",
    "        print(\"Retrieved %d posts from page %s\" % (count, after))\n",
    "        return data\n",
    "    \n",
    "    after = \"\"\n",
    "\n",
    "    x, y = [], []\n",
    "    for n in range(N//count):\n",
    "        data = _get_data(subreddit, count, after)\n",
    "        for d in data['data']['children']:\n",
    "            x.append(d['data']['num_comments'])\n",
    "            y.append(d['data']['score'])\n",
    "        after = data['data']['after']\n",
    "\n",
    "    return x, y\n",
    "                          \n",
    "x, y = get_x_y(\"blackmirror\", 500, count=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.1.3**: There is clearly a huge level unevenness in the distribution of how likes and comments given to different posts. Let's visualize this using histograms!\n",
    "1. Log transform `y` (e.g. create a new variable called `y_transformed`) and input it to `plt.hist`. Notice that if there are zeros in `y`, `np.log` will convert them to `-inf`, which `plt.hist` can't handle. In this case, we will remove zeros before log transforming. When you have done this, execute `hist_output = plt.hist(y_transformed)`. This should produce a histogram. But what does the variable `hist_output` contain?\n",
    "2. Use `hist_output` to make a similar histogram with the `plt.bar` plotting function. I make you do this to force\n",
    "into your permanent memory what a histogram is: a bar chart showing counts within intervals/bins.\n",
    "3. Plot the distributions of `y_transformed` for \"blackmirror\" and \"news\" as histograms, side by side (you can just use the regular `plt.hist` function here). My figure looks like [this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_2.2c.png). Comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.1**: What does Joel (book) mean when he uses the word *vector*? What are [Grant](https://youtu.be/fNk_zzaMoSs)s vector definitions from the perspectives of the Physicist, the Computer Scientist and the Mathematician, respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.2**: Using `numpy`, compute:\n",
    "1. `2 * [2, 3]`,\n",
    "2. `[3, 8] + [6, 1]`,\n",
    "3. `[3, 8] * [6, 1]` and\n",
    "4. `[3, 8] · [6, 1]` (dot product)\n",
    "5. `[3, 8, 0] x [6, 1, 0]` (cross product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.3**: Say you have two vectors. What does it mean that the dot product between them is zero or very close to zero? What if it's very large? Intuitively, what does the dot product then measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.4**: In Data Science, we often think of matrices as (usually two-dimensional) containers for data. If we have $N=500$ data points each with $M$ features to them, we can represent this data using an $N \\times M$ matrix, that is a matrix that has $N$ rows, one for each datapoint, and $M$ columns, one for each feature. Below I fetch a dataset of wines (rows) and their features (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T09:53:11.903738Z",
     "start_time": "2019-11-11T09:53:11.593708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic.acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Acl</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid.phenols</th>\n",
       "      <th>Proanth</th>\n",
       "      <th>Color.int</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.39</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.45</td>\n",
       "      <td>14.6</td>\n",
       "      <td>96</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.02</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.06</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.61</td>\n",
       "      <td>17.6</td>\n",
       "      <td>121</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.05</td>\n",
       "      <td>1.06</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.83</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13.86</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.27</td>\n",
       "      <td>16.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.85</td>\n",
       "      <td>7.22</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol  Malic.acid   Ash   Acl   Mg  Phenols  Flavanoids  \\\n",
       "0    14.23        1.71  2.43  15.6  127     2.80        3.06   \n",
       "1    13.20        1.78  2.14  11.2  100     2.65        2.76   \n",
       "2    13.16        2.36  2.67  18.6  101     2.80        3.24   \n",
       "3    14.37        1.95  2.50  16.8  113     3.85        3.49   \n",
       "4    13.24        2.59  2.87  21.0  118     2.80        2.69   \n",
       "5    14.20        1.76  2.45  15.2  112     3.27        3.39   \n",
       "6    14.39        1.87  2.45  14.6   96     2.50        2.52   \n",
       "7    14.06        2.15  2.61  17.6  121     2.60        2.51   \n",
       "8    14.83        1.64  2.17  14.0   97     2.80        2.98   \n",
       "9    13.86        1.35  2.27  16.0   98     2.98        3.15   \n",
       "\n",
       "   Nonflavanoid.phenols  Proanth  Color.int   Hue    OD  Proline  \n",
       "0                  0.28     2.29       5.64  1.04  3.92     1065  \n",
       "1                  0.26     1.28       4.38  1.05  3.40     1050  \n",
       "2                  0.30     2.81       5.68  1.03  3.17     1185  \n",
       "3                  0.24     2.18       7.80  0.86  3.45     1480  \n",
       "4                  0.39     1.82       4.32  1.04  2.93      735  \n",
       "5                  0.34     1.97       6.75  1.05  2.85     1450  \n",
       "6                  0.30     1.98       5.25  1.02  3.58     1290  \n",
       "7                  0.31     1.25       5.05  1.06  3.58     1295  \n",
       "8                  0.29     1.98       5.20  1.08  2.85     1045  \n",
       "9                  0.22     1.85       7.22  1.01  3.55     1045  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Download dataset\n",
    "X = pd.read_csv(\"https://gist.githubusercontent.com/tijptjik/9408623/raw/b237fa5848349a14a14e5d4107dc7897c21951f5/wine.csv\").drop('Wine', axis=1)\n",
    "\n",
    "# Display dataset\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">So this dataset has $N=178$ rows and $M=13$ columns. Let's start by finding the so-called *covariance matrix* of the features. It is a square, in this case, $13\\times13$ matrix where every value $i,j$ scores the covariance between features $i$ and $j$. [Read more here](https://en.wikipedia.org/wiki/Covariance_matrix).\n",
    "1. Use the `np.cov` method on `X` to get its $13 \\times 13$ covariance matrix.\n",
    "2. Plot the covariance matrix using `plt.imshow` and `plt.colorbar()`.\n",
    "3. Plot the correlations in the same way. Comment on the differences between these two plots. Is one easier to interpret than the other?\n",
    ">\n",
    ">*Hint 1: `np.cov` expects that rows are features and columns are observations. That is the transpose of how `X` \n",
    "is represented now.*<br>\n",
    ">*Hint 2: The correlation matrix can be obtained with the `np.corrcoef` function.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.5**: There's another use of the covariance matrix, other than just learning how features co-vary. In fact, it turns out that the *eigenvectors* of the covariance matrix are a set of mutually orthogonal vectors, that point in the directions of greatest variance in the data. The eigenvector with the greatest *eigenvalue* points along the direction of greatest variation, and so on. This is pretty neat, because if we know along which axes the data is most stretched, we can figure out how best to project it when visualizing it in 2D as a scatter plot! This whole procedure has a name: **Principal Component Analysis** (PCA) and it was invented by Karl Pearson in 1901. It belongs to a powerful class of linear algebra methods called **Matrix Factorization** methods. Ok, so rather than spending too much time on the math of PCA, let's just use the `sklearn` implementation and fit a PCA on `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>46.5</td>\n",
       "      <td>17.9</td>\n",
       "      <td>192.0</td>\n",
       "      <td>3500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>50.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>196.0</td>\n",
       "      <td>3900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>51.3</td>\n",
       "      <td>19.2</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>45.4</td>\n",
       "      <td>18.7</td>\n",
       "      <td>188.0</td>\n",
       "      <td>3525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>52.7</td>\n",
       "      <td>19.8</td>\n",
       "      <td>197.0</td>\n",
       "      <td>3725.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>47.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>214.0</td>\n",
       "      <td>4925.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>46.8</td>\n",
       "      <td>14.3</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>50.4</td>\n",
       "      <td>15.7</td>\n",
       "      <td>222.0</td>\n",
       "      <td>5750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>45.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>212.0</td>\n",
       "      <td>5200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>49.9</td>\n",
       "      <td>16.1</td>\n",
       "      <td>213.0</td>\n",
       "      <td>5400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n",
       "152            46.5           17.9              192.0       3500.0\n",
       "153            50.0           19.5              196.0       3900.0\n",
       "154            51.3           19.2              193.0       3650.0\n",
       "155            45.4           18.7              188.0       3525.0\n",
       "156            52.7           19.8              197.0       3725.0\n",
       "..              ...            ...                ...          ...\n",
       "338            47.2           13.7              214.0       4925.0\n",
       "340            46.8           14.3              215.0       4850.0\n",
       "341            50.4           15.7              222.0       5750.0\n",
       "342            45.2           14.8              212.0       5200.0\n",
       "343            49.9           16.1              213.0       5400.0\n",
       "\n",
       "[187 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the Penguin dataset through the seaborn library\n",
    "import seaborn as sns\n",
    "df = sns.load_dataset('penguins'); df = df[df['species']!='Adelie']; df = df.dropna(); X = df[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']]\n",
    "colors = df['species'].map({'Adelie':'C0','Chinstrap':'C1','Gentoo':'C2'})\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T09:54:12.788440Z",
     "start_time": "2019-11-11T09:54:12.761895Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1. Explain what the matrix you get when you call `pca.components_` means.\n",
    "2. Make a bar plot of `pca.explained_variance_ratio_` and explain what it means (you could log-scale the y-axis to visualize both large and small values). What insights about our data can we extract from this?\n",
    "3. Indeed, problem with the data AS-IS, is that the different features have very different orders of magnitude (some are huge numbers others are small). The way to fix this is by doing something called \"[zscoring](https://en.wikipedia.org/wiki/Standard_score)\", whereby each feature is normalized/rescaled to have zero mean and unit standard deviation. In this way, all of the data ends up with comparable variance. Make a new array `X_z` that is the zscored `X`, using the `scipy.stats.zscore` function. Show that each column has zero mean and unit standard deviation.\n",
    "4. Transform `X` using the PCA we fitted above to create a new array `X_pca`. Then fit a new PCA to `X_z` and transform it to create another new array `X_z_pca`. Produce the bar plot from before and comment on the differences.\n",
    "5. Finally, scatter plot against each other the first two components (i.e. first two columns in the array) of `X_pca`, and color the data by penguin type. Do the same for `X_z_pca`. Comment on the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Statistics (DSFS Chapter 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.3.1**: Take a vector `a = [1, 3, 2, 5, 3, 1, 5, 1, 9000]`:\n",
    "1. Compute the mean of `a` using `numpy`.\n",
    "2. How is median defined? Compute the median of `a` using `numpy`.\n",
    "3. For `a`, why might it make sense to take the median more seriously than the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.3.2**: Using the same vector `a`:\n",
    "1. How is *range* defined? Compute it.\n",
    "2. How is *variance* defined? How do variance and standard deviation relate? Compute them both. Which value is greater?\n",
    "3. What is the interquartile range? Compute it, and explain why it might be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.3.3**: Covariance and correlation are both measures of trend similarity.\n",
    "1. How do they relate?\n",
    "2. Compute the correlation between `a` and `b = [0, 4, 1, 6, 2, 0, 6, 0, 2]`.\n",
    "3. How does that result change if you remove the last data-point from each list? Why? What *term* do we use for that last value for both `a` and `b`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
